---
title: "LLM 기초 (24/05/02)"
layout: single
---

> LLM과 RAG 이해

## 1. 자연어 처리
* [강의자료][1-1]
* [OpenAI Key 발급 방법][1-2]
* [자연어 이해][1-3]
* [Instruct GPT][1-4]
* [Prompt-Baed Learning][1-5]
* [Embeddings][1-6]
* [ChatGPT Token][1-7]
* [ChatGPT Pricing][1-8]
* [ChatGPT 기사][1-9]
* [BloombergGPT-1][1-10]
* [BloombergGPT-2][1-11]
* [BloombergGPT-3][1-12]
* [Hugging Face][1-13]
* [Code Alpaca][1-14]
* [RAG][1-15]
* [초록마을][1-16]
* [Sentence Similarity][1-17]
* [Azure][1-18]
* [LangChain-1][1-19]
* [LangChain-2][1-20]
* [LangChain 국세 예제][1-21]

## 2. LLM 모델
* [강의자료][2-1]
* [Hugging Face 토큰 발급 방법][2-2]
* [Transformer][2-3]
* [Attention][2-4]
* [Bert와 GPT][2-5]
* [Language Model][2-6]
* [LLaMA][2-7]
* [GPT-4 Size][2-8]
* [LLMOps][2-9]
* [LLM Models][2-10]
* [LLaMA-3][2-11]
* [SKTGPT-2 예제][2-12]
* [LangChain 예제][2-13]
* [Completion][2-14]
* [LangChain 예제][2-15]
* [Optimization][2-16]
* [Quantization][2-17]
* [Fine-tuning][2-18]
* [QLoRA][2-19]
* [종합소득세 예제][2-20]
* [LangChain][2-21]

[1-1]: https://drive.google.com/file/d/1oT7mn3KjRAy7AlGYMln-oLLGk7nHj5wU/view
[1-2]: https://drive.google.com/file/d/1oYLOBR-8HER-eFZEGYMSZUE5xk61cvJI/view
[1-3]: https://woongsin94.tistory.com/341
[1-4]: https://www.theinsaneapp.com/2023/05/everything-about-instructgpt.html
[1-5]: https://developers.reinfer.io/blog/2022/05/04/prompting
[1-6]: https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c
[1-7]: https://www.makeuseof.com/what-is-chatgpt-token-limit-can-you-exceed-it/
[1-8]: https://openai.com/api/pricing
[1-9]: https://m.mk.co.kr/news/it/10868530
[1-10]: https://www.linkedin.com/pulse/what-i-learned-from-bloombergs-experience-building-own-chanen-phd/
[1-11]: https://www.youtube.com/watch?v=3ZqJaL1jJN4
[1-12]: https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/
[1-13]: https://huggingface.co/docs/transformers/training
[1-14]: https://github.com/sahil280114/codealpaca
[1-15]: https://m.kmib.co.kr/view_amp.asp?arcid=0924293095
[1-16]: https://news.mt.co.kr/mtview.php?no=2023081010234353171
[1-17]: https://huggingface.co/tasks/sentence-similarity
[1-18]: https://github.com/Azure-Samples/azure-search-openai-demo
[1-19]: https://medium.com/sopmac-ai/chatgpt-langchain-example-for-chatbot-q-a-a8b6ef40bbb6
[1-20]: https://www.geeksforgeeks.org/build-chatbot-webapp-with-langchain/
[1-21]: https://drive.google.com/file/d/1p6p35PAhI-7u_ROmQ94pdV2eJzOn093u/view?usp=sharing
[2-1]: https://drive.google.com/file/d/1oLTtvJKyHmHp-uZDRJbA-lmAMSQnPhG8/view
[2-2]: https://drive.google.com/file/d/1oadlBRBUlhznPbjdD3WEecuW3xHIIlkm/view
[2-3]: https://www.fandom.com/articles/how-transformers-captured-out-attention
[2-4]: https://arxiv.org/pdf/1706.03762
[2-5]: https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/
[2-6]: https://lifearchitect.ai/models/
[2-7]: https://s10251.pcdn.co/wp-content/uploads/2023/07/2023-llama-0628-final-big.png
[2-8]: https://lifearchitect.ai/gpt-4/#size
[2-9]: https://www.youtube.com/watch?v=Fquj2u7ay40
[2-10]: https://huggingface.co/papers/2307.09793
[2-11]: https://modulabs.co.kr/blog/llama-3-intro/
[2-12]: https://drive.google.com/file/d/1oqCYo-uqV0llwMGjlLdrAjHoKNhovV4g/view?usp=sharing
[2-13]: https://drive.google.com/file/d/1oww9wY9tETovJyRwlBNIsYIPt8GtP4jN/view?usp=sharing
[2-14]: https://platform.openai.com/docs/guides/text-generation/completions-api
[2-15]: https://drive.google.com/file/d/1qZh8M9I_xi9Rdi-R7QZE0msalgMxOfgY/view?usp=sharing
[2-16]: https://medium.com/@satya15july_11937/network-optimization-with-quantization-8-bit-vs-1-bit-af2fd716fcae
[2-17]: https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996
[2-18]: https://xiaosean5408.medium.com/fine-tuning-llms-made-easy-with-lora-and-generative-ai-stable-diffusion-lora-39ff27480fda
[2-19]: https://arxiv.org/pdf/2305.14314
[2-20]: https://drive.google.com/file/d/1qbO1_pweX_p5w6FxSmuxzZvSIuknsm-q/view?usp=sharing
[2-21]: https://knowslog.tistory.com/entry/Langchain%EC%9C%BC%EB%A1%9C-LLaMA2-cpp-%EB%B2%84%EC%A0%84-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0
